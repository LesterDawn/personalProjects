import os
from nltk.stem.porter import *
import numpy as np
import re


def load_dataset(data_dir):
    """
    :param data_dir: data set absolute directory
    :return: dataset as a whole string
    """
    wordStr = ""
    for subdir in os.listdir(data_dir):
        for file in os.listdir(data_dir + "\\" + subdir):
            f = open(data_dir + "\\" + subdir + "\\" + file, 'r', encoding='Latin1')
            wordStr += f.read()
    return wordStr


def word_processing(wordStr):
    """
    Split the words and reserve the words with ', e.g. you'll, he's...
    :param wordStr: dataset string
    :return: words list without non-alphabets letters
    """
    return [i for i in re.split("[^a-zA-Z\']", wordStr) if i != '']


def stop_words_processing(stop_words_dir, words):
    """
    First removal: remove all the stopwords
    Second removal: remove the new stopwords generated by word splitting (split with ')
    :param stop_words_dir: stop words absolute directory
    :param words: processed words
    :return: words list without stop words
    """
    stop_words_list = [i.lower() for i in open(stop_words_dir, 'r', encoding='Latin1').read().split("\n")]
    stop_words = set(stop_words_list)
    temp = [item for item in words if item.lower() not in stop_words]
    wordStr = " ".join(temp)
    word_list = re.sub(r'[^a-z]', ' ', wordStr.lower()).strip().split(' ')
    return [item for item in word_list if item.lower() not in stop_words]


def word_stemming(words):
    """
    :param words: words without stop words
    :return: stems set
    """
    stemmer = PorterStemmer()
    return set([i for i in [stemmer.stem(plural) for plural in words] if i != ''])


def document_process(data_dir):
    """
    :param data_dir: dataset directory
    :return: document vectors stored as list
    """
    docs = []
    stemmer = PorterStemmer()
    for subdir in os.listdir(data_dir):
        for file in os.listdir(data_dir + "\\" + subdir):
            f = open(data_dir + "\\" + subdir + "\\" + file, 'r', encoding='Latin1')
            contents = f.read()
            temp = re.sub(r'[^a-z]', ' ', contents.lower().strip()).split(" ")
            pure_contents = [i for i in temp if i != '']
            stem_contents = [stemmer.stem(plural) for plural in pure_contents]
            docs.append(stem_contents)
    return docs


def word_frequency_cal(docs, num_file):
    """
    :param docs: document vectors
    :param num_file: number of documents
    :return: word frequency dictionary
    """
    word_freq = {word: np.zeros(num_file, dtype=int) for word in stems}
    i = 0
    for doc in docs:
        for word in doc:
            if word in stems:
                word_freq[word][i] += 1
        i += 1
    return word_freq


def doc_frequency_cal(docs):
    """
    :param docs: document vectors
    :return: document frequency dictionary
    """
    doc_freq = {word: 0 for word in stems}
    for doc in docs:
        for word in set(doc):
            if word in stems:
                doc_freq[word] += 1
    return doc_freq


def weight_cal(word_freq, doc_freq, num_file):
    """
    :param word_freq: word frequency dictionary (fik)
    :param doc_freq: document frequency dictionary (nk)
    :param num_file: number of documents
    :return: weight of word (aik)
    """
    weight = {word: np.zeros(num_file, dtype=int) for word in stems}
    for word in weight:
        weight[word] = word_freq[word] * np.log2(num_file / doc_freq[word])
    return weight


def weight_normalization(weight, num_file):
    """
    :param weight: weight of word (aik)
    :param num_file: number of files
    :return: normalized weight (Aik)
    """
    matrix_2D = np.array(list(weight.values())).reshape(num_file, len(stems))
    norm_weight = matrix_2D / np.power(np.sum(np.power(matrix_2D, 2), axis=1, keepdims=True), 0.5)
    return norm_weight


# prepare the directories being loaded
dataset_directory = 'E:\XJTLU\INT401-Fundamentals of ML\Labs\Assignment1\lab2-feature-generation\dataset'
stop_words_directory = 'E:\XJTLU\INT401-Fundamentals of ML\Labs\Assignment1\lab2-feature-generation\stopwords.txt'
save_directory = 'E:\XJTLU\INT401-Fundamentals of ML\Labs\Assignment1\\train-20ng.npz'

# process the documents to obtain doc vectors
word_string = load_dataset(dataset_directory)
processed_words = word_processing(word_string)
no_stop_words = stop_words_processing(stop_words_directory, processed_words)
stems = word_stemming(no_stop_words)
documents = document_process(dataset_directory)
num_files = len(documents)

# calculate fik, N and nk to get aik and Aik (TFIDF matrix)
word_frequency = word_frequency_cal(documents, num_files)
document_frequency = doc_frequency_cal(documents)
word_weight = weight_cal(word_frequency, document_frequency, num_files)
TFIDF_matrix = weight_normalization(word_weight, num_files)

# save the result matrix
np.savez(save_directory, X=TFIDF_matrix)
